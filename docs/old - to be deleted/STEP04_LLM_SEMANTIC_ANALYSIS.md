# STEP04: LLM Semantic Analysis Implementation

**Version:** 1.0  
**Date:** July 22, 2025  
**Purpose:** Detailed implementation specification for Large Language Model semantic analysis and business logic extraction

---

## üìã Step Overview

### **Primary Responsibility**
Large Language Model semantic analysis for business logic extraction, component description generation, domain classification, and functional requirement enhancement through AI-powered code understanding.

### **Processing Context**
- **Pipeline Position:** Fourth step in the CodeSight pipeline
- **Dependencies:** STEP01 (file inventory), STEP02 (AST structure), STEP03 (configuration analysis)
- **Processing Time:** 40-50% of total pipeline time
- **Confidence Level:** 65-75% (AI-powered semantic analysis)

### **Data Flow Integration**

#### **Data Requirements from Previous Steps:**

**From STEP01 (File System Analysis):**
- **Project metadata and context** - Project name, architecture patterns for LLM context
- **Directory structure analysis** - Package organization for domain understanding
- **Project-specific architectural patterns** - Custom patterns (asl, dsl, gsl, isl) for enhanced semantic context

**From STEP02 (AST Structural Extraction):**
- **Method signatures and business logic indicators** - For semantic enhancement
- **Component classifications with confidence** - For LLM validation and refinement
- **Class structures and relationships** - For context-aware analysis
- **All code samples referenced by Unix relative paths**

**From STEP03 (Pattern & Configuration Analysis):**
- **Configuration context** - Framework settings for semantic enhancement
- **Architectural patterns identified** - MVC compliance, DI patterns for LLM context
- **Business rules from configuration** - Validation rules, constraints for semantic analysis
- **Environment-specific configurations** - Development vs production patterns for context

#### **Data Provided to Subsequent Steps:**

**For STEP05 (Relationship Mapping):**
- **Enhanced component descriptions** - Business purpose and functionality for relationship context
- **Domain classifications** - Business domain groupings for logical relationship mapping
- **Functional requirements** - Detailed business rules for dependency validation
- **Component confidence adjustments** - LLM-validated confidence scores

**For STEP07 (Output Generation & Validation):**
- **Complete functional requirements** - All business logic extracted and documented
- **Enhanced component metadata** - Domain, subdomain, descriptions for final schema
- **Quality indicators** - LLM analysis confidence for validation metrics

---

## ‚öôÔ∏è Design Principles

### **Project and Language Agnostic Design**
- **All LLM prompts must be language and project agnostic where possible**
- **Business logic analysis patterns are configurable through prompt templates**
- **Domain classification rules are defined in configuration, not hard-coded**
- **LLM model selection and parameters are configurable per project**
- **Code analysis prompts support multiple programming languages through templates**

### **Standardized Path Handling**
- **All code samples sent to LLM must reference Unix-style relative paths**
- **File references in LLM output must maintain Unix relative path format**
- **No absolute paths are permitted in LLM prompts or responses**
- **Path normalization is applied before and after LLM processing**
- **All file references must be traceable to original Unix relative paths**

### **Configuration System Compliance**
- **Inherits dual YAML configuration system from previous steps**
- **LLM configuration parameters are defined in project-specific YAML**
- **Prompt templates support project-specific customization**
- **Custom domain classification rules per project through configuration**

---

## üéØ Requirements Alignment

### **Functional Requirements Covered**
- **FR-003: Business Rule Extraction** - AI-powered business logic identification and documentation
- **FR-004: Relationship Mapping** - Enhanced component understanding for relationship analysis
- **FR-005: Project and Language Agnostic Design** - All code must be project and language agnostic
- **FR-006: Standardized Path Handling** - All paths must be relative to root project directory using Unix format
- **FR-007: Project-Specific Architectural Overrides** - Support custom semantic analysis per project

### **Target Schema Attributes**
Based on SCHEMA_ATTRIBUTE_MAPPING.md, STEP04 extracts:

| Attribute | Confidence | Method |
|-----------|------------|--------|
| `components[].domain` | 70% | LLM semantic analysis + package structure hints |
| `components[].subdomain` | 60% | LLM semantic analysis of component purpose |
| `components[].description` | 65% | LLM-generated component description from code analysis |
| `components[].functional_requirements.business_rules` | 70% | AI-powered business logic extraction |
| `components[].functional_requirements.description` | 75% | LLM-enhanced functional descriptions |
| `components[].technical_requirements.complexity_indicators` | 65% | AI analysis of code complexity and patterns |
| `metadata.architecture_patterns.primary_pattern` | 75% | LLM pattern recognition enhancement |

---

## üì• Input Specifications

### **Primary Inputs**
- **STEP01 Output** - `step01_output.json` with project context and file inventory
- **STEP02 Output** - `step02_output.json` with component structure and code analysis
- **STEP03 Output** - `step03_output.json` with configuration context and patterns
- **Source Code Access** - Direct access to source files for code sample extraction
- **YAML Configuration** - LLM-specific configuration parameters

### **Configuration Requirements**

#### **Enhanced config.yaml (Common LLM Configuration)**
```yaml
# LLM Configuration
llm_analysis:
  provider: "openai"  # openai, anthropic, azure-openai, local
  model: "gpt-4"
  max_tokens: 4000
  temperature: 0.1
  timeout_seconds: 30
  retry_attempts: 3
  
  # Code analysis parameters
  max_code_sample_lines: 100
  context_window_lines: 20
  include_comments: true
  include_imports: false

# Prompt Templates (Language-Agnostic)
prompt_templates:
  component_analysis: |
    Analyze this {language} code component and provide:
    1. Business domain classification
    2. Component description (1-2 sentences)
    3. Primary business purpose
    4. Key business rules identified
    
    Code file: {file_path}
    Component type: {component_type}
    Project context: {project_context}
    
    Code sample:
    ```{language}
    {code_sample}
    ```
    
    Respond in JSON format with keys: domain, description, business_purpose, business_rules[]
  
  business_logic_extraction: |
    Extract business rules and logic from this {language} code:
    
    File: {file_path}
    Method/Class: {target_element}
    
    Code:
    ```{language}
    {code_sample}
    ```
    
    Identify:
    1. Validation rules (required fields, formats, ranges)
    2. Business calculations or formulas
    3. Decision logic and conditions
    4. Data transformation rules
    5. Error handling patterns
    
    Return JSON with: validation_rules[], calculations[], decisions[], transformations[]

  domain_classification: |
    Based on the package structure and component names, classify this component into a business domain:
    
    Package: {package_name}
    Component: {component_name}
    File path: {file_path}
    
    Available domains: {available_domains}
    Custom domains allowed: true
    
    Consider:
    - Package naming conventions
    - Class/file naming patterns
    - Business context from code
    
    Return JSON with: domain, subdomain, confidence, reasoning

# Domain Classification Rules
domain_classification:
  default_domains:
    - "User_Management"
    - "Content_Management" 
    - "Security_Authorization"
    - "Data_Processing"
    - "Integration_Services"
    - "UI_Presentation"
    - "Business_Logic"
    - "Infrastructure"
  
  package_patterns:
    - pattern: "*user*"
      domain: "User_Management"
      confidence_boost: 0.3
    - pattern: "*auth*"
      domain: "Security_Authorization" 
      confidence_boost: 0.3
    - pattern: "*content*"
      domain: "Content_Management"
      confidence_boost: 0.3
    - pattern: "*ui*|*web*|*controller*"
      domain: "UI_Presentation"
      confidence_boost: 0.2

# Business Rule Extraction Patterns
business_rule_patterns:
  validation_indicators:
    - "validate"
    - "check"
    - "verify"
    - "required"
    - "mandatory"
  calculation_indicators:
    - "calculate"
    - "compute"
    - "sum"
    - "total"
    - "rate"
    - "percentage"
  decision_indicators:
    - "if"
    - "switch"
    - "decide"
    - "determine"
    - "select"
```

#### **Enhanced config-<project>.yaml (Project-Specific LLM Configuration)**
```yaml
# Project-Specific LLM Configuration
project_llm_settings:
  domain_context: "Enterprise HR and Storm system for NBCUniversal"
  business_context: "Legacy application modernization for cloud migration"
  
  # Custom domain classifications for Storm project
  custom_domains:
    - "HR_Employee_Management"
    - "Time_Tracking" 
    - "Storm_Core_Services"
    - "Storm_Application_Layer"
    - "Storm_Domain_Layer"
    - "Storm_Generic_Layer"
    - "Storm_Infrastructure_Layer"
  
  # Custom architectural layer analysis (asl, dsl, gsl, isl)
  architectural_semantic_analysis:
    asl_patterns:
      domain_hint: "Storm_Application_Layer"
      business_focus: "Application-specific business logic and workflow coordination"
      analysis_emphasis: "workflow patterns, application orchestration, user interaction logic"
    dsl_patterns:
      domain_hint: "Storm_Domain_Layer" 
      business_focus: "Core domain business rules and entity management"
      analysis_emphasis: "business rules, domain validation, entity relationships"
    gsl_patterns:
      domain_hint: "Storm_Generic_Layer"
      business_focus: "Reusable utilities and cross-cutting concerns"
      analysis_emphasis: "utility functions, common patterns, shared resources"
    isl_patterns:
      domain_hint: "Storm_Infrastructure_Layer"
      business_focus: "External system integration and infrastructure concerns"
      analysis_emphasis: "system integration, external APIs, infrastructure patterns"

# Enhanced prompt context for project
project_prompt_enhancements:
  context_additions:
    - "This is a legacy Storm/HR system being modernized"
    - "Focus on HR domain concepts like employees, time tracking, benefits"
    - "Identify Storm-specific architectural patterns (asl, dsl, gsl, isl)"
    - "Consider NBCUniversal enterprise context"
  
  domain_specific_terms:
    - "employee"
    - "timesheet" 
    - "payroll"
    - "benefits"
    - "storm"
    - "hr"
    - "nbcu"
```

---

## üì§ Output Specifications

### **Output File: step04_output.json**
AI-enhanced semantic analysis with business intelligence:

```json
{
    "step_metadata": {
        "step_name": "llm_semantic_analysis",
        "execution_timestamp": "ISO 8601 timestamp",
        "processing_time_ms": "integer",
        "components_analyzed": "integer",
        "llm_calls_made": "integer",
        "total_tokens_used": "integer",
        "errors_encountered": "integer",
        "configuration_sources": ["config.yaml", "config-<project>.yaml"]
    },
    "llm_configuration": {
        "provider": "string",
        "model": "string",
        "parameters": "object",
        "prompt_templates_used": ["array"],
        "analysis_scope": "object"
    },
    "enhanced_components": [{
        "component_name": "string",
        "original_type": "string",                      // From STEP02
        "enhanced_type": "string",                      // LLM-validated type
        "confidence_adjustment": "float -1 to 1",       // LLM confidence adjustment
        "domain_classification": {
            "domain": "string",
            "subdomain": "string",
            "confidence": "float 0-1",
            "reasoning": "string",
            "package_context": "string",
            "architectural_context": "string"          // Custom patterns (asl, dsl, gsl, isl)
        },
        "business_analysis": {
            "description": "string",                    // 1-2 sentence component description
            "business_purpose": "string",               // Primary business function
            "complexity_assessment": "string",          // low, medium, high
            "modernization_priority": "string",         // low, medium, high, critical
            "technical_debt_indicators": ["array"]
        },
        "functional_requirements_enhanced": {
            "business_rules": [{
                "rule_id": "string",
                "rule_type": "string",                  // validation, calculation, decision, transformation
                "description": "string",
                "confidence": "float 0-1",
                "source_file": "string",               // Unix relative path
                "source_method": "string",
                "complexity": "string",                // simple, moderate, complex
                "business_impact": "string"            // low, medium, high
            }],
            "data_validations": [{
                "field_name": "string",
                "validation_type": "string",
                "validation_rule": "string",
                "error_condition": "string",
                "business_rationale": "string",
                "source_file": "string",               // Unix relative path
                "confidence": "float 0-1"
            }],
            "business_calculations": [{
                "calculation_name": "string",
                "description": "string",
                "formula_description": "string",
                "input_parameters": ["array"],
                "output_type": "string",
                "business_context": "string",
                "source_file": "string",               // Unix relative path
                "confidence": "float 0-1"
            }],
            "decision_logic": [{
                "decision_point": "string",
                "condition_description": "string",
                "possible_outcomes": ["array"],
                "business_rationale": "string",
                "source_file": "string",               // Unix relative path
                "confidence": "float 0-1"
            }]
        },
        "architectural_insights": {
            "design_patterns": ["array"],
            "dependencies_identified": ["array"],
            "integration_points": ["array"],
            "modernization_recommendations": ["array"]
        },
        "code_quality_analysis": {
            "maintainability_score": "float 0-1",
            "complexity_score": "float 0-1",
            "documentation_quality": "string",          // poor, fair, good, excellent
            "test_coverage_indicators": "string",
            "refactoring_suggestions": ["array"]
        }
    }],
    "domain_analysis": {
        "identified_domains": [{
            "domain_name": "string",
            "component_count": "integer",
            "business_description": "string",
            "architectural_significance": "string",
            "modernization_complexity": "string",
            "components": ["array"]                     // Component names in this domain
        }],
        "cross_domain_relationships": [{
            "source_domain": "string",
            "target_domain": "string",
            "relationship_type": "string",
            "interaction_description": "string",
            "integration_complexity": "string"
        }],
        "business_process_flows": [{
            "process_name": "string",
            "description": "string",
            "domains_involved": ["array"],
            "components_involved": ["array"],
            "business_criticality": "string"
        }]
    },
    "architectural_analysis": {
        "overall_architecture_assessment": {
            "primary_pattern": "string",
            "pattern_adherence": "float 0-1",
            "architectural_debt": "string",              // low, medium, high
            "modernization_readiness": "string"          // poor, fair, good, excellent
        },
        "custom_architectural_patterns": {              // Project-specific patterns (asl, dsl, gsl, isl)
            "asl_analysis": {
                "components_count": "integer", 
                "business_focus": "string",
                "complexity_assessment": "string",
                "modernization_recommendations": ["array"]
            },
            "dsl_analysis": {
                "components_count": "integer",
                "business_focus": "string", 
                "complexity_assessment": "string",
                "modernization_recommendations": ["array"]
            },
            "gsl_analysis": {
                "components_count": "integer",
                "business_focus": "string",
                "complexity_assessment": "string", 
                "modernization_recommendations": ["array"]
            },
            "isl_analysis": {
                "components_count": "integer",
                "business_focus": "string",
                "complexity_assessment": "string",
                "modernization_recommendations": ["array"]
            }
        },
        "integration_complexity": {
            "external_dependencies": ["array"],
            "data_integration_patterns": ["array"],
            "service_integration_complexity": "string"
        }
    },
    "business_intelligence": {
        "key_business_processes": [{
            "process_name": "string",
            "components_involved": ["array"],
            "business_value": "string",                 // low, medium, high, critical
            "automation_level": "string",               // manual, semi-automated, automated
            "modernization_opportunity": "string"
        }],
        "data_flow_patterns": [{
            "flow_name": "string",
            "source_components": ["array"],
            "target_components": ["array"],
            "data_transformation": "string",
            "business_impact": "string"
        }],
        "compliance_requirements": [{
            "requirement_type": "string",
            "description": "string",
            "affected_components": ["array"],
            "compliance_level": "string"
        }]
    },
    "modernization_insights": {
        "microservices_candidates": [{
            "service_name": "string",
            "components": ["array"],
            "business_capability": "string",
            "independence_score": "float 0-1",
            "migration_complexity": "string"
        }],
        "cloud_readiness": {
            "overall_score": "float 0-1",
            "blocking_factors": ["array"],
            "enablers": ["array"],
            "recommended_approach": "string"
        },
        "technical_debt_analysis": {
            "high_debt_components": ["array"],
            "debt_categories": ["array"],
            "remediation_priority": ["array"]
        }
    },
    "quality_metrics": {
        "llm_analysis_confidence": "float 0-1",
        "component_analysis_success_rate": "float 0-1", 
        "business_rule_extraction_completeness": "float 0-1",
        "domain_classification_accuracy": "float 0-1",
        "cross_validation_success": "float 0-1",
        "analysis_coverage": "float 0-1"
    },
    "validation_results": {
        "llm_response_quality": "float 0-1",
        "semantic_analysis_consistency": "float 0-1",
        "business_logic_coverage": "float 0-1",
        "domain_classification_confidence": "float 0-1",
        "architectural_insights_accuracy": "float 0-1",
        "issues": ["array"]
    }
}
```

---

## üîß Implementation Details

### **Phase 1: LLM Configuration and Setup**
1. **LLM Provider Configuration**
   ```python
   def configure_llm_provider(config):
       provider_config = config['llm_analysis']
       
       if provider_config['provider'] == 'openai':
           return OpenAIProvider(
               model=provider_config['model'],
               max_tokens=provider_config['max_tokens'],
               temperature=provider_config['temperature']
           )
       elif provider_config['provider'] == 'anthropic':
           return AnthropicProvider(
               model=provider_config['model'],
               max_tokens=provider_config['max_tokens']
           )
       # Language-agnostic provider abstraction
       else:
           raise ValueError(f"Unsupported LLM provider: {provider_config['provider']}")
   ```

2. **Prompt Template Management**
   ```python
   def load_prompt_templates(config):
       templates = config['prompt_templates']
       
       # Language-agnostic template loading
       processed_templates = {}
       for template_name, template_content in templates.items():
           processed_templates[template_name] = {
               'content': template_content,
               'parameters': extract_template_parameters(template_content),
               'language_agnostic': True
           }
       
       return processed_templates
   ```

3. **Code Sample Extraction (Language-Agnostic)**
   ```python
   def extract_code_samples(component, step02_output, config):
       code_samples = []
       max_lines = config['llm_analysis']['max_code_sample_lines']
       context_lines = config['llm_analysis']['context_window_lines']
       
       for file_info in component['files']:
           file_path = file_info['path']  # Unix relative path from STEP02
           
           # Extract relevant code sections based on component type
           if component['component_type'] == 'service':
               samples = extract_business_logic_sections(file_path, max_lines, context_lines)
           elif component['component_type'] == 'screen':
               samples = extract_ui_logic_sections(file_path, max_lines, context_lines)
           else:
               samples = extract_general_code_sections(file_path, max_lines, context_lines)
           
           code_samples.extend(samples)
       
       return code_samples
   ```

### **Phase 2: Component Analysis and Enhancement**
1. **Domain Classification**
   ```python
   def classify_component_domain(component, project_context, config):
       prompt_template = config['prompt_templates']['domain_classification']
       
       # Prepare context for LLM
       package_name = extract_package_from_path(component['files'][0]['path'])
       available_domains = config['domain_classification']['default_domains']
       
       # Add project-specific domains
       if 'custom_domains' in project_context:
           available_domains.extend(project_context['custom_domains'])
       
       # Check for architectural context (asl, dsl, gsl, isl patterns)
       architectural_context = determine_architectural_context(component, project_context)
       
       prompt = prompt_template.format(
           package_name=package_name,
           component_name=component['name'],
           file_path=component['files'][0]['path'],  # Unix relative path
           available_domains=available_domains,
           architectural_context=architectural_context
       )
       
       llm_response = call_llm(prompt)
       domain_result = parse_domain_response(llm_response)
       
       # Apply confidence boost for package patterns
       confidence_boost = calculate_package_pattern_boost(package_name, config)
       domain_result['confidence'] = min(domain_result['confidence'] + confidence_boost, 1.0)
       
       return domain_result
   ```

2. **Business Logic Extraction**
   ```python
   def extract_business_logic(component, code_samples, config):
       business_rules = []
       
       for code_sample in code_samples:
           prompt_template = config['prompt_templates']['business_logic_extraction']
           
           prompt = prompt_template.format(
               language=detect_language_from_path(code_sample['file_path']),
               file_path=code_sample['file_path'],  # Unix relative path
               target_element=code_sample['target_element'],
               code_sample=code_sample['code']
           )
           
           llm_response = call_llm(prompt)
           extracted_rules = parse_business_logic_response(llm_response)
           
           # Enhance rules with file context
           for rule in extracted_rules:
               rule['source_file'] = code_sample['file_path']  # Unix relative path
               rule['confidence'] = calculate_rule_confidence(rule, code_sample)
           
           business_rules.extend(extracted_rules)
       
       return business_rules
   ```

3. **Component Description Generation**
   ```python
   def generate_component_description(component, domain_info, code_samples, config):
       prompt_template = config['prompt_templates']['component_analysis']
       
       # Prepare comprehensive context
       project_context = config.get('project_llm_settings', {}).get('business_context', '')
       
       # Determine language from first file
       language = detect_language_from_path(component['files'][0]['path'])
       
       # Combine code samples for analysis
       combined_code = combine_code_samples(code_samples, max_length=2000)
       
       prompt = prompt_template.format(
           language=language,
           file_path=component['files'][0]['path'],  # Unix relative path
           component_type=component['component_type'],
           project_context=project_context,
           code_sample=combined_code
       )
       
       llm_response = call_llm(prompt)
       analysis_result = parse_component_analysis_response(llm_response)
       
       return {
           'description': analysis_result.get('description', ''),
           'business_purpose': analysis_result.get('business_purpose', ''),
           'complexity_assessment': assess_complexity(combined_code, analysis_result),
           'modernization_priority': determine_modernization_priority(component, analysis_result)
       }
   ```

### **Phase 3: Architectural Pattern Analysis**
1. **Custom Architectural Pattern Analysis (asl, dsl, gsl, isl)**
   ```python
   def analyze_custom_architectural_patterns(components, project_config):
       pattern_analysis = {}
       architectural_config = project_config.get('architectural_semantic_analysis', {})
       
       for pattern_type in ['asl', 'dsl', 'gsl', 'isl']:
           if pattern_type + '_patterns' in architectural_config:
               pattern_config = architectural_config[pattern_type + '_patterns']
               
               # Find components matching this pattern
               matching_components = []
               for component in components:
                   if component_matches_pattern(component, pattern_type):
                       matching_components.append(component)
               
               if matching_components:
                   pattern_analysis[pattern_type + '_analysis'] = {
                       'components_count': len(matching_components),
                       'business_focus': pattern_config.get('business_focus', ''),
                       'complexity_assessment': analyze_pattern_complexity(matching_components),
                       'modernization_recommendations': generate_pattern_recommendations(
                           matching_components, pattern_config
                       )
                   }
       
       return pattern_analysis
   ```

2. **Cross-Domain Relationship Analysis**
   ```python
   def analyze_cross_domain_relationships(enhanced_components):
       relationships = []
       domains = group_components_by_domain(enhanced_components)
       
       for source_domain, source_components in domains.items():
           for target_domain, target_components in domains.items():
               if source_domain != target_domain:
                   relationship = analyze_domain_interaction(
                       source_domain, source_components,
                       target_domain, target_components
                   )
                   if relationship:
                       relationships.append(relationship)
       
       return relationships
   ```

### **Phase 4: Modernization Analysis**
1. **Microservices Candidate Identification**
   ```python
   def identify_microservices_candidates(enhanced_components, domain_analysis):
       candidates = []
       
       for domain in domain_analysis['identified_domains']:
           if domain['component_count'] >= 2:  # Minimum viable service size
               independence_score = calculate_independence_score(
                   domain['components'], enhanced_components
               )
               
               if independence_score > 0.6:  # Threshold for viable microservice
                   candidates.append({
                       'service_name': generate_service_name(domain['domain_name']),
                       'components': domain['components'],
                       'business_capability': domain['business_description'],
                       'independence_score': independence_score,
                       'migration_complexity': assess_migration_complexity(domain['components'])
                   })
       
       return candidates
   ```

2. **Cloud Readiness Assessment**
   ```python
   def assess_cloud_readiness(enhanced_components, architectural_analysis):
       blocking_factors = []
       enablers = []
       
       # Analyze stateful components
       for component in enhanced_components:
           if has_local_state(component):
               blocking_factors.append(f"Stateful component: {component['component_name']}")
           
           if has_cloud_patterns(component):
               enablers.append(f"Cloud-ready patterns in: {component['component_name']}")
       
       # Calculate overall readiness score
       total_factors = len(blocking_factors) + len(enablers)
       if total_factors > 0:
           readiness_score = len(enablers) / total_factors
       else:
           readiness_score = 0.5  # Neutral score
       
       return {
           'overall_score': readiness_score,
           'blocking_factors': blocking_factors,
           'enablers': enablers,
           'recommended_approach': determine_migration_approach(readiness_score)
       }
   ```

### **Phase 5: Quality Validation and Cross-Reference**
1. **LLM Response Quality Validation**
   ```python
   def validate_llm_response_quality(llm_responses):
       quality_scores = []
       
       for response in llm_responses:
           score = 0.0
           
           # Check response completeness
           if response.get('description'):
               score += 0.2
           if response.get('business_rules'):
               score += 0.3
           if response.get('domain'):
               score += 0.2
           
           # Check response consistency
           if validate_response_consistency(response):
               score += 0.3
           
           quality_scores.append(score)
       
       return sum(quality_scores) / len(quality_scores) if quality_scores else 0.0
   ```

2. **Cross-Validation with Previous Steps**
   ```python
   def cross_validate_with_previous_steps(enhanced_components, step02_output, step03_output):
       validation_results = []
       
       for enhanced_comp in enhanced_components:
           original_comp = find_original_component(enhanced_comp['component_name'], step02_output)
           
           if original_comp:
               # Validate component type consistency
               type_consistency = validate_type_consistency(
                   original_comp['component_type'], 
                   enhanced_comp['enhanced_type']
               )
               
               # Validate business rules against configuration
               config_consistency = validate_against_config(
                   enhanced_comp['functional_requirements_enhanced']['business_rules'],
                   step03_output
               )
               
               validation_results.append({
                   'component': enhanced_comp['component_name'],
                   'type_consistency': type_consistency,
                   'config_consistency': config_consistency,
                   'overall_consistency': (type_consistency + config_consistency) / 2
               })
       
       return validation_results
   ```

---

## ‚úÖ Validation and Quality Assurance

### **Internal Validation**
1. **LLM Response Validation**
   ```python
   def validate_llm_responses():
       response_quality_metrics = {
           'completeness': 0.0,
           'consistency': 0.0,
           'relevance': 0.0,
           'accuracy': 0.0
       }
       
       for response in all_llm_responses:
           # Validate response structure
           if validate_json_structure(response):
               response_quality_metrics['completeness'] += 1
           
           # Check semantic consistency
           if validate_semantic_consistency(response):
               response_quality_metrics['consistency'] += 1
           
           # Validate business relevance
           if validate_business_relevance(response):
               response_quality_metrics['relevance'] += 1
       
       total_responses = len(all_llm_responses)
       for metric in response_quality_metrics:
           response_quality_metrics[metric] /= total_responses
       
       return response_quality_metrics
   ```

2. **Business Logic Extraction Quality**
   ```python
   def validate_business_logic_quality():
       extracted_rules = collect_all_business_rules()
       
       quality_indicators = {
           'rules_with_context': 0,
           'rules_with_validation': 0,
           'rules_with_high_confidence': 0
       }
       
       for rule in extracted_rules:
           if rule.get('business_rationale'):
               quality_indicators['rules_with_context'] += 1
           if rule.get('validation_rule'):
               quality_indicators['rules_with_validation'] += 1
           if rule.get('confidence', 0) > 0.7:
               quality_indicators['rules_with_high_confidence'] += 1
       
       total_rules = len(extracted_rules)
       return {
           indicator: count / total_rules 
           for indicator, count in quality_indicators.items()
       } if total_rules > 0 else {}
   ```

3. **Domain Classification Accuracy**
   ```python
   def validate_domain_classification():
       domain_assignments = collect_domain_assignments()
       
       # Check for domain distribution reasonableness
       domain_counts = count_components_per_domain(domain_assignments)
       
       # Validate against package structure hints
       package_consistency = validate_package_domain_consistency(domain_assignments)
       
       # Check for architectural pattern consistency
       architectural_consistency = validate_architectural_domain_consistency(
           domain_assignments, detected_custom_patterns
       )
       
       return {
           'distribution_balance': calculate_distribution_balance(domain_counts),
           'package_consistency': package_consistency,
           'architectural_consistency': architectural_consistency,
           'overall_accuracy': (package_consistency + architectural_consistency) / 2
       }
   ```

### **Output Validation**
1. **JSON Schema Compliance**
   ```python
   def validate_step04_output_schema():
       required_sections = [
           'step_metadata', 'enhanced_components', 'domain_analysis',
           'architectural_analysis', 'business_intelligence', 'modernization_insights',
           'quality_metrics', 'validation_results'
       ]
       
       for section in required_sections:
           if section not in step04_output:
               raise ValueError(f"Missing required section: {section}")
       
       # Validate Unix relative path compliance
       validate_unix_relative_paths_in_llm_output(step04_output)
       
       return True
   ```

2. **Enhanced Component Quality**
   ```python
   def validate_enhanced_component_quality():
       enhanced_components = step04_output['enhanced_components']
       
       quality_metrics = {
           'components_with_descriptions': 0,
           'components_with_domains': 0,
           'components_with_business_rules': 0,
           'high_confidence_enhancements': 0
       }
       
       for component in enhanced_components:
           if component.get('business_analysis', {}).get('description'):
               quality_metrics['components_with_descriptions'] += 1
           
           if component.get('domain_classification', {}).get('domain'):
               quality_metrics['components_with_domains'] += 1
           
           if component.get('functional_requirements_enhanced', {}).get('business_rules'):
               quality_metrics['components_with_business_rules'] += 1
           
           if component.get('confidence_adjustment', 0) > 0.0:
               quality_metrics['high_confidence_enhancements'] += 1
       
       total_components = len(enhanced_components)
       return {
           metric: count / total_components 
           for metric, count in quality_metrics.items()
       } if total_components > 0 else {}
   ```

### **Success Criteria**
- **LLM Response Quality**: 80%+ of LLM responses meet quality thresholds
- **Component Analysis Coverage**: 95%+ of components receive enhanced analysis
- **Business Rule Extraction Success**: 70%+ of identifiable business rules extracted
- **Domain Classification Accuracy**: 75%+ accuracy in domain assignment
- **Cross-Step Validation Success**: 85%+ consistency with previous step findings
- **Path Format Compliance**: 100% of file references must be Unix relative format
- **Custom Pattern Recognition**: 80%+ accuracy in identifying project-specific patterns

---

## üö® Error Handling

### **LLM API Failures**
```python
def handle_llm_api_failure(component, error, retry_count):
    error_info = {
        "component": component['name'],
        "error_type": type(error).__name__,
        "error_message": str(error),
        "retry_count": retry_count,
        "timestamp": datetime.now().isoformat()
    }
    log_error(error_info)
    
    if retry_count < 3:
        # Exponential backoff retry
        time.sleep(2 ** retry_count)
        return retry_llm_analysis(component, retry_count + 1)
    else:
        # Fallback to basic analysis without LLM
        return create_fallback_analysis(component)
```

### **Response Parsing Failures**
```python
def handle_response_parsing_failure(llm_response, component):
    # Attempt alternative parsing strategies
    try:
        # Try relaxed JSON parsing
        parsed = parse_relaxed_json(llm_response)
        return parsed
    except:
        # Extract partial information with regex
        partial_info = extract_partial_info(llm_response)
        
        # Log parsing failure
        log_parsing_failure(component['name'], llm_response)
        
        # Return minimal analysis structure
        return create_minimal_analysis(component, partial_info)
```

### **Quality Threshold Failures**
```python
def handle_quality_threshold_failure(analysis_result, quality_score):
    if quality_score < 0.5:
        # Mark analysis as low confidence
        analysis_result['confidence_adjustment'] = -0.2
        analysis_result['quality_warning'] = "Low quality LLM analysis"
        
        # Request human review flag
        analysis_result['requires_human_review'] = True
        
        log_quality_warning(analysis_result)
    
    return analysis_result
```

---

## üìä Performance Considerations

### **Optimization Strategies**
1. **Batch LLM Processing**
   ```python
   def batch_llm_analysis(components, batch_size=5):
       batches = create_component_batches(components, batch_size)
       results = []
       
       for batch in batches:
           # Process batch in parallel where possible
           batch_results = process_component_batch(batch)
           results.extend(batch_results)
           
           # Rate limiting for API calls
           time.sleep(1)  # Respect API rate limits
       
       return results
   ```

2. **Code Sample Optimization**
   ```python
   def optimize_code_samples(component, max_tokens=3000):
       # Prioritize business logic methods
       # Remove boilerplate and generated code
       # Focus on key business methods identified in STEP02
       # Truncate samples to stay within token limits
       pass
   ```

3. **Caching and Memoization**
   ```python
   def cache_llm_responses():
       # Cache responses for similar code patterns
       # Reuse domain classifications for similar packages
       # Store common business rule patterns
       pass
   ```

### **Resource Limits**
- **LLM Token Limits**: 4000 tokens per request (configurable)
- **Processing Time**: Target <45 minutes for typical projects
- **API Rate Limits**: Respect provider-specific rate limits
- **Memory Usage**: Target <1GB for component analysis
- **Retry Limits**: Maximum 3 retries per failed LLM call

---

## üîç Testing Strategy

### **Unit Tests**
1. **LLM Integration Tests**
   ```python
   def test_llm_provider_integration():
       # Test LLM provider configuration
       # Verify prompt template formatting
       # Test response parsing accuracy
   
   def test_domain_classification():
       # Test domain assignment accuracy
       # Verify custom domain support
       # Test architectural pattern influence
   ```

2. **Business Logic Extraction Tests**
   ```python
   def test_business_rule_extraction():
       # Test validation rule identification
       # Verify calculation logic extraction
       # Test decision logic recognition
   
   def test_component_enhancement():
       # Test description generation quality
       # Verify confidence adjustment accuracy
       # Test modernization insight generation
   ```

### **Integration Tests**
1. **Cross-Step Validation Tests**
   ```python
   def test_step02_step04_integration():
       # Test consistency with AST analysis
       # Verify component type validation
       # Test business rule cross-validation
   ```

2. **End-to-End LLM Analysis**
   ```python
   def test_complete_llm_analysis():
       # Test full STEP04 pipeline with real components
       # Verify all output requirements are met
       # Test error handling and recovery
   ```

---

## üìù Configuration Examples

#### **Development Environment**
**config.yaml enhancements**:
```yaml
llm_analysis:
  provider: "openai"
  model: "gpt-3.5-turbo"  # Faster, cheaper for development
  temperature: 0.2
  max_tokens: 2000
  timeout_seconds: 15

# Simplified prompts for development
prompt_templates:
  component_analysis: "Analyze this code component briefly: {code_sample}"
```

#### **Production Environment**
**config-storm.yaml enhancements**:
```yaml
project_llm_settings:
  domain_context: "NBCUniversal Storm HR System - Legacy enterprise application"
  business_context: "Enterprise HR system managing employee data, time tracking, and payroll processing for NBCUniversal. System uses custom architectural layers (asl, dsl, gsl, isl) and requires modernization for cloud migration."
  
  custom_domains:
    - "HR_Employee_Management"
    - "Time_Tracking_System"
    - "Payroll_Processing"
    - "Benefits_Administration"
    - "Storm_Application_Services"
    - "Storm_Domain_Services"
    - "Storm_Generic_Utilities"
    - "Storm_Infrastructure_Services"

  architectural_semantic_analysis:
    asl_patterns:
      domain_hint: "Storm_Application_Services"
      business_focus: "Application-specific workflow coordination, user interface integration, session management"
      analysis_emphasis: "user workflows, application orchestration, UI business logic"
    dsl_patterns:
      domain_hint: "Storm_Domain_Services"
      business_focus: "Core HR business rules, employee data validation, payroll calculations"
      analysis_emphasis: "business validation, domain rules, entity management"
    gsl_patterns:
      domain_hint: "Storm_Generic_Utilities"
      business_focus: "Shared utilities, common functions, cross-cutting infrastructure"
      analysis_emphasis: "utility patterns, helper functions, shared resources"
    isl_patterns:
      domain_hint: "Storm_Infrastructure_Services"
      business_focus: "Database integration, external system connectivity, infrastructure concerns"
      analysis_emphasis: "data access, system integration, external APIs"
```
